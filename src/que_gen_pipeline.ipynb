{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "que_gen_pipeline",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "78tWYiI3z4pU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "outputId": "d57a7e6f-b7f3-461b-f54d-7274c0cf6b02"
      },
      "source": [
        "!gdown -O  t5_que_gen.zip --id 1vhsDOW9wUUO83IQasTPlkxb82yxmMH-V\n",
        "!unzip t5_que_gen.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1vhsDOW9wUUO83IQasTPlkxb82yxmMH-V\n",
            "To: /content/t5_que_gen.zip\n",
            "1.65GB [00:23, 71.0MB/s]\n",
            "Archive:  t5_que_gen.zip\n",
            "   creating: t5_que_gen_model/\n",
            "   creating: t5_que_gen_model/t5_base_tok_que_gen/\n",
            "  inflating: t5_que_gen_model/t5_base_tok_que_gen/spiece.model  \n",
            " extracting: t5_que_gen_model/t5_base_tok_que_gen/added_tokens.json  \n",
            " extracting: t5_que_gen_model/t5_base_tok_que_gen/tokenizer_config.json  \n",
            "  inflating: t5_que_gen_model/t5_base_tok_que_gen/special_tokens_map.json  \n",
            "   creating: t5_que_gen_model/t5_base_que_gen/\n",
            "  inflating: t5_que_gen_model/t5_base_que_gen/config.json  \n",
            "  inflating: t5_que_gen_model/t5_base_que_gen/pytorch_model.bin  \n",
            " extracting: t5_que_gen_model/logs.zip  \n",
            "   creating: t5_ans_gen_model/\n",
            "   creating: t5_ans_gen_model/t5_base_tok_ans_gen/\n",
            "  inflating: t5_ans_gen_model/t5_base_tok_ans_gen/spiece.model  \n",
            "  inflating: t5_ans_gen_model/t5_base_tok_ans_gen/added_tokens.json  \n",
            " extracting: t5_ans_gen_model/t5_base_tok_ans_gen/tokenizer_config.json  \n",
            "  inflating: t5_ans_gen_model/t5_base_tok_ans_gen/special_tokens_map.json  \n",
            "   creating: t5_ans_gen_model/t5_base_ans_gen/\n",
            "  inflating: t5_ans_gen_model/t5_base_ans_gen/config.json  \n",
            "  inflating: t5_ans_gen_model/t5_base_ans_gen/pytorch_model.bin  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmpn1fnWU_Ye",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        },
        "outputId": "b0b3e173-4f4f-4b4b-fc83-80eccc562dba"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/b5/ac41e3e95205ebf53439e4dd087c58e9fd371fd8e3724f2b9b4cdb8282e5/transformers-2.10.0-py3-none-any.whl (660kB)\n",
            "\r\u001b[K     |▌                               | 10kB 16.5MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 4.5MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30kB 6.2MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 7.9MB/s eta 0:00:01\r\u001b[K     |██▌                             | 51kB 5.0MB/s eta 0:00:01\r\u001b[K     |███                             | 61kB 5.9MB/s eta 0:00:01\r\u001b[K     |███▌                            | 71kB 6.6MB/s eta 0:00:01\r\u001b[K     |████                            | 81kB 7.4MB/s eta 0:00:01\r\u001b[K     |████▌                           | 92kB 6.0MB/s eta 0:00:01\r\u001b[K     |█████                           | 102kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 112kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████                          | 122kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 133kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████                         | 143kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 153kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████                        | 163kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 174kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████                       | 184kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 194kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████                      | 204kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 215kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████                     | 225kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 235kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████                    | 245kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 256kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 266kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 276kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 286kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 296kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 307kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 317kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 327kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 337kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 348kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 358kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 368kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 378kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 389kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 399kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 409kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 419kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 430kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 440kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 450kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 460kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 471kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 481kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 491kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 501kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 512kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 522kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 532kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 542kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 552kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 563kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 573kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 583kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 593kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 604kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 614kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 624kB 6.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 634kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 645kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 655kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 665kB 6.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 38.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 34.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 37.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=035b2a52d2e73680bdb69260d0acc42c5054a1107ce5e0220fe1862f14ef176b\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.10.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0osYhHHVJM3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "4d0d672b-f19b-460b-afe6-dd7ebed18bc3"
      },
      "source": [
        "import argparse\n",
        "import glob\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import logging\n",
        "import random\n",
        "from itertools import chain\n",
        "from string import punctuation\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    T5ForConditionalGeneration,\n",
        "    T5Tokenizer,\n",
        "    get_linear_schedule_with_warmup\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "um5HzLKCj-hf"
      },
      "source": [
        "class QueGenerator():\n",
        "  def __init__(self):\n",
        "    self.que_model = T5ForConditionalGeneration.from_pretrained('./t5_que_gen_model/t5_base_que_gen/')\n",
        "    self.ans_model = T5ForConditionalGeneration.from_pretrained('./t5_ans_gen_model/t5_base_ans_gen/')\n",
        "\n",
        "    self.que_tokenizer = T5Tokenizer.from_pretrained('./t5_que_gen_model/t5_base_tok_que_gen/')\n",
        "    self.ans_tokenizer = T5Tokenizer.from_pretrained('./t5_ans_gen_model/t5_base_tok_ans_gen/')\n",
        "    \n",
        "    self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    \n",
        "    self.que_model = self.que_model.to(self.device)\n",
        "    self.ans_model = self.ans_model.to(self.device)\n",
        "  \n",
        "  def generate(self, text):\n",
        "    answers = self._get_answers(text)\n",
        "    questions = self._get_questions(text, answers)\n",
        "    output = [{'answer': ans, 'question': que} for ans, que in zip(answers, questions)]\n",
        "    return output\n",
        "  \n",
        "  def _get_answers(self, text):\n",
        "    # split into sentences\n",
        "    sents = sent_tokenize(text)\n",
        "\n",
        "    examples = []\n",
        "    for i in range(len(sents)):\n",
        "      input_ = \"\"\n",
        "      for j, sent in enumerate(sents):\n",
        "        if i == j:\n",
        "            sent = \"[HL] %s [HL]\" % sent\n",
        "        input_ = \"%s %s\" % (input_, sent)\n",
        "        input_ = input_.strip()\n",
        "      input_ = input_ + \" </s>\"\n",
        "      examples.append(input_)\n",
        "    \n",
        "    batch = self.ans_tokenizer.batch_encode_plus(examples, max_length=512, pad_to_max_length=True, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "      outs = self.ans_model.generate(input_ids=batch['input_ids'].to(self.device), \n",
        "                                attention_mask=batch['attention_mask'].to(self.device), \n",
        "                                max_length=32,\n",
        "                                # do_sample=False,\n",
        "                                # num_beams = 4,\n",
        "                                )\n",
        "    dec = [self.ans_tokenizer.decode(ids, skip_special_tokens=False) for ids in outs]\n",
        "    answers = [item.split('[SEP]') for item in dec]\n",
        "    answers = chain(*answers)\n",
        "    answers = [ans.strip() for ans in answers if ans != ' ']\n",
        "    return answers\n",
        "  \n",
        "  def _get_questions(self, text, answers):\n",
        "    examples = []\n",
        "    for ans in answers:\n",
        "      input_text = \"%s [SEP] %s </s>\" % (ans, text)\n",
        "      examples.append(input_text)\n",
        "    \n",
        "    batch = self.que_tokenizer.batch_encode_plus(examples, max_length=512, pad_to_max_length=True, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "      outs = self.que_model.generate(input_ids=batch['input_ids'].to(self.device), \n",
        "                                attention_mask=batch['attention_mask'].to(self.device), \n",
        "                                max_length=32,\n",
        "                                num_beams = 4)\n",
        "    dec = [self.que_tokenizer.decode(ids, skip_special_tokens=False) for ids in outs]\n",
        "    return dec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsHUUqC8nWzf"
      },
      "source": [
        "que_generator = QueGenerator()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Jw33J0Kfxbd"
      },
      "source": [
        "text = \"Python is an interpreted, high-level, general-purpose programming language. Created by Guido van Rossum \\\n",
        "and first released in 1991, Python's design philosophy emphasizes code \\\n",
        "readability with its notable use of significant whitespace.\"\n",
        "\n",
        "text2 = \"Gravity (from Latin gravitas, meaning 'weight'), or gravitation, is a natural phenomenon by which all \\\n",
        "things with mass or energy—including planets, stars, galaxies, and even light—are brought toward (or gravitate toward) \\\n",
        "one another. On Earth, gravity gives weight to physical objects, and the Moon's gravity causes the ocean tides. \\\n",
        "The gravitational attraction of the original gaseous matter present in the Universe caused it to begin coalescing \\\n",
        "and forming stars and caused the stars to group together into galaxies, so gravity is responsible for many of \\\n",
        "the large-scale structures in the Universe. Gravity has an infinite range, although its effects become increasingly \\\n",
        "weaker as objects get further away\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JR_yWhFCsUev",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "60fb73ea-af86-41e3-d6c7-e4ddd5cf1d6f"
      },
      "source": [
        "que_generator.generate(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'answer': 'Python',\n",
              "  'question': 'What is the name of the interpreted, high-level, general-purpose programming language?'},\n",
              " {'answer': 'Guido van Rossum', 'question': 'Who created Python?'},\n",
              " {'answer': '1991', 'question': 'When was Python released?'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCQ1GHPX0we8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "91926f55-5f6c-407c-fae0-78dcf4c5c67d"
      },
      "source": [
        "que_generator.generate(text2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'answer': 'weight', 'question': 'What does gravitas mean in English?'},\n",
              " {'answer': 'Earth',\n",
              "  'question': 'On what planet does gravity give weight to physical objects?'},\n",
              " {'answer': 'galaxies', 'question': 'What do the stars form together into?'},\n",
              " {'answer': 'weaker',\n",
              "  'question': \"What do gravity's effects become as objects get further away?\"}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KZlZgVB0xTu"
      },
      "source": [
        "tetx = \"A dentist, also known as a dental surgeon, is a surgeon who specializes in dentistry, the diagnosis, prevention, and treatment of diseases and conditions of the oral cavity. The dentist's supporting team aids in providing oral health services. The dental team includes dental assistants, dental hygienists, dental technicians, and sometimes dental therapists.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SE9PTXMured",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "cde55e78-24e4-425c-8621-aff75bdbff33"
      },
      "source": [
        "que_generator.generate(tetx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'answer': 'dental surgeon',\n",
              "  'question': 'What is another name for a dentist?'},\n",
              " {'answer': \"The dentist's supporting team\",\n",
              "  'question': 'Who provides oral health services?'},\n",
              " {'answer': 'dental assistants, dental hygienists, dental technicians, and sometimes dental therapists',\n",
              "  'question': 'What is a dental team comprised of?'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlt3CgE7uuMz"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}